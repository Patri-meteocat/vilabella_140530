{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## You are using the Python ARM Radar Toolkit (Py-ART), an open source\n",
      "## library for working with weather radar data. Py-ART is partly\n",
      "## supported by the U.S. Department of Energy as part of the Atmospheric\n",
      "## Radiation Measurement (ARM) Climate Research Facility, an Office of\n",
      "## Science user facility.\n",
      "##\n",
      "## If you use this software to prepare a publication, please cite:\n",
      "##\n",
      "##     JJ Helmus and SM Collis, JORS 2016, doi: 10.5334/jors.119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as plb\n",
    "import matplotlib as mpl\n",
    "import pyart\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import numpy.ma as ma\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from pylab import *\n",
    "from scipy import ndimage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy_cols(data, kernel, val='nan'):\n",
    "    \n",
    "    # Image boundaries are wrapped for calculation of convolution\n",
    "    # NA values (zeroes/nan) need to be added in range\n",
    "    \n",
    "    c = (np.asarray(kernel.shape)-1)/2\n",
    "    add_cols = ceil(c[1])\n",
    "    dummy_cols = np.zeros((data.shape[0], add_cols.astype(int)))\n",
    "    \n",
    "    if val=='nan':\n",
    "        dummy_cols[:] = np.NAN\n",
    "    else:\n",
    "        dummy_cols[:] = val\n",
    "        \n",
    "    # Add dummy columns\n",
    "    data_out = np.hstack((data, dummy_cols))\n",
    "    \n",
    "    return add_cols, data_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def primary_vel(dim, Nprf, prf_flag=None, prf_odd=None):\n",
    "    \n",
    "    ## Construct array with the dual-PRF factor corresponding to each gate ##\n",
    "    # Flag 0 indicates low PRF and flag 1 indicates high PRF\n",
    "    \n",
    "    flag_vec = np.ones(dim[0])\n",
    "    \n",
    "    if (prf_flag is None) & (prf_odd is not None) :\n",
    "        if prf_odd==0:\n",
    "            flag_vec[::2] = 0\n",
    "        elif prf_odd==1:\n",
    "            flag_vec[1::2] = 0\n",
    "    else:\n",
    "        flag_vec = prf_flag\n",
    "        \n",
    "    flag_vec = flag_vec - 1\n",
    "    flag_vec[flag_vec<0] = 1\n",
    "        \n",
    "    flag_arr = np.transpose(np.tile(flag_vec, (dim[1], 1)))\n",
    "    Nprf_arr = flag_arr + Nprf\n",
    "    \n",
    "    return Nprf_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ref_val(data, mask, kernel, method='mean'):\n",
    "    \n",
    "    dummy_val=0\n",
    "    Nval_arr = np.ones(mask.shape)\n",
    "    \n",
    "    if method=='mean':\n",
    "        Nval_arr = local_valid(mask, kernel=kernel)\n",
    "        dummy_data = data*(~mask).astype(int)\n",
    "        \n",
    "    if method=='median':\n",
    "        dummy_val='nan'\n",
    "        dummy_data = np.where(np.logical_not(mask), data, np.nan)\n",
    "        \n",
    "\n",
    "    ncols, data_conv = dummy_cols(dummy_data, kernel, val=dummy_val)\n",
    "    \n",
    "    if method=='mean':\n",
    "        conv_arr = ndimage.convolve(data_conv, weights=kernel, mode='wrap')\n",
    "        \n",
    "    if method=='median':\n",
    "        conv_arr = ndimage.generic_filter(data_conv, np.nanmedian, footprint=kernel, mode='wrap')\n",
    "    \n",
    "    # Remove added columns and divide by weight\n",
    "    conv_arr = conv_arr[:, : int(conv_arr.shape[1] - ncols)]\n",
    "    ref_arr = conv_arr/Nval_arr\n",
    "    \n",
    "    return ref_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outlier_detector_cmean(np_ma, Vny, Nprf_arr, Nmin=2):\n",
    "    \n",
    "    data = np_ma.data\n",
    "    mask = np_ma.mask\n",
    "    \n",
    "    f_arr = np.ones(Nprf_arr.shape)\n",
    "    f_arr[np.where(Nprf_arr==np.min(Nprf_arr))] = -1\n",
    "    Vny_arr = Vny/Nprf_arr\n",
    "    \n",
    "    kH, kL = np.zeros((5,5)), np.zeros((5,5))\n",
    "    kH[1::2] = 1\n",
    "    kL[::2] = 1\n",
    "     \n",
    "    # Array with the number of valid neighbours at each point\n",
    "    Nval_arr_H = local_valid(mask, kernel=kH)\n",
    "    Nval_arr_L = local_valid(mask, kernel=kL)\n",
    "    \n",
    "    # Convert to angles and calculate trigonometric variables\n",
    "    ang_ma = (np_ma*pi/Vny)\n",
    "    cos_ma = ma.cos(ang_ma*Nprf_arr)\n",
    "    sin_ma = ma.sin(ang_ma*Nprf_arr)\n",
    "    \n",
    "    # Average trigonometric variables in local neighbourhood\n",
    "    dummy_cos = cos_ma.data*(~mask).astype(int)\n",
    "    dummy_sin = sin_ma.data*(~mask).astype(int)\n",
    "    \n",
    "    ncols, cos_conv = dummy_cols(dummy_cos, kH, val=0)\n",
    "    ncols, sin_conv = dummy_cols(dummy_sin, kH, val=0)\n",
    "    \n",
    "    cos_sumH = ndimage.convolve(cos_conv, weights=kH, mode='wrap')\n",
    "    cos_sumL = ndimage.convolve(cos_conv, weights=kL, mode='wrap')\n",
    "    \n",
    "    sin_sumH = ndimage.convolve(sin_conv, weights=kH, mode='wrap')\n",
    "    sin_sumL = ndimage.convolve(sin_conv, weights=kL, mode='wrap')\n",
    "    \n",
    "    # Remove added columns\n",
    "    cos_sumH = cos_sumH[:, : int(cos_sumL.shape[1] - ncols)]\n",
    "    cos_sumL = cos_sumL[:, : int(cos_sumL.shape[1] - ncols)]\n",
    "    sin_sumH = sin_sumH[:, : int(sin_sumL.shape[1] - ncols)]\n",
    "    sin_sumL = sin_sumL[:, : int(sin_sumL.shape[1] - ncols)]\n",
    "    \n",
    "    # Average angle in local neighbourhood\n",
    "    cos_avgH_ma = ma.array(data=cos_sumH, mask=mask)/Nval_arr_H\n",
    "    cos_avgL_ma = ma.array(data=cos_sumL, mask=mask)/Nval_arr_L\n",
    "    sin_avgH_ma = ma.array(data=sin_sumH, mask=mask)/Nval_arr_H\n",
    "    sin_avgL_ma = ma.array(data=sin_sumL, mask=mask)/Nval_arr_L\n",
    "      \n",
    "    BH = ma.arctan2(sin_avgH_ma, cos_avgH_ma)\n",
    "    BL = ma.arctan2(sin_avgL_ma, cos_avgL_ma)\n",
    "    \n",
    "    # Average velocity ANGLE of neighbours (reference ANGLE for outlier detection):\n",
    "    angref_ma = f_arr*(BL-BH)\n",
    "    angref_ma[angref_ma<0] = angref_ma[angref_ma<0] + 2*pi\n",
    "    angref_ma[angref_ma>pi] = - (2*pi - angref_ma[angref_ma>pi])\n",
    "    angobs_ma = ma.arctan2(ma.sin(ang_ma), ma.cos(ang_ma))\n",
    "    \n",
    "    # Detector array (minimum ANGLE difference between observed and reference):\n",
    "    diff = angobs_ma - angref_ma\n",
    "    det_ma = (Vny/pi)*ma.arctan2(ma.sin(diff), ma.cos(diff))\n",
    "    \n",
    "    out_mask = np.zeros(det_ma.shape)\n",
    "    out_mask[abs(det_ma)>0.8*Vny_arr] = 1\n",
    "    out_mask[(Nval_arr_H<Nmin)|(Nval_arr_L<Nmin)] = 0\n",
    "    \n",
    "    # CORRECTION (2 STEP)\n",
    "    \n",
    "    # Convolution kernel\n",
    "    kernel = np.ones(kH.shape)\n",
    "    \n",
    "    new_mask = (mask) | (out_mask.astype(bool))\n",
    "    \n",
    "    # Array with the number of valid neighbours at each point (outliers removed)\n",
    "    Nval_arr = local_valid(new_mask, kernel=kernel)\n",
    "    \n",
    "    out_mask[Nval_arr<Nmin] = 0\n",
    "    \n",
    "    ref_arr = ref_val(data, new_mask, kernel, method='median')\n",
    "    ref_ma = ma.array(data=ref_arr, mask=mask)\n",
    "    \n",
    "    return ref_ma, out_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_dualPRF_cmean(radar, field='velocity', Nprf=3, \n",
    "                   corr_method='median', Nmin=2):\n",
    "    \n",
    "    v_ma = radar.fields[field]['data']\n",
    "    vcorr_ma = v_ma.copy()\n",
    "    out_mask = np.zeros(v_ma.shape)\n",
    "    \n",
    "    # Dual-PRF parameters\n",
    "    Vny = radar.instrument_parameters['nyquist_velocity']['data'][0]\n",
    "    prf_flag = radar.instrument_parameters['prf_flag']['data']\n",
    "    \n",
    "    # Array with the primary Nyquist velocity corresponding to each bin\n",
    "    Nprf_arr = primary_vel(v_ma.shape, Nprf, prf_flag=prf_flag)\n",
    "    Vny_arr = Vny/Nprf_arr\n",
    "    \n",
    "    for nsweep, sweep_slice in enumerate(radar.iter_slice()):\n",
    "        \n",
    "        v0 = v_ma[sweep_slice] # velocity field\n",
    "        vp = Vny_arr[sweep_slice] # primary velocities\n",
    "        nprfp = Nprf_arr[sweep_slice] # dual-PRF factors\n",
    "        \n",
    "        ref, out_mask[sweep_slice] = outlier_detector_cmean(v0, Vny, nprfp, Nmin=Nmin)\n",
    "                 \n",
    "        # Convert non-outliers to zero for correction procedure  \n",
    "        v0_out = v0*out_mask[sweep_slice]\n",
    "        vp_out = vp*out_mask[sweep_slice]\n",
    "        ref_out = ref*out_mask[sweep_slice]\n",
    "        vp_out_L = vp_out.copy() # Only low PRF outliers\n",
    "        vp_out_L[nprfp==Nprf] = 0\n",
    "        \n",
    "        dev = ma.abs(v0_out-ref_out)\n",
    "        nuw = np.zeros(v0.shape) # Number of unwraps (initialisation)\n",
    "        \n",
    "        for ni in range(-Nprf, (Nprf+1)):\n",
    "            \n",
    "            # New velocity values for identified outliers\n",
    "            if abs(ni)==Nprf:\n",
    "                vcorr_out = v0_out + 2*ni*vp_out_L\n",
    "            else:\n",
    "                vcorr_out = v0_out + 2*ni*vp_out\n",
    "            \n",
    "            # New deviation for new velocity values\n",
    "            dev_tmp = ma.abs(vcorr_out-ref_out)\n",
    "            # Compare with previous\n",
    "            delta = dev-dev_tmp\n",
    "            # Update unwrap number\n",
    "            nuw[delta>0] = ni\n",
    "            # Update corrected velocity and deviation\n",
    "            vcorr_out_tmp = v0_out + 2*nuw*vp_out\n",
    "            dev = ma.abs(vcorr_out_tmp-ref_out)\n",
    "            \n",
    "        # Corrected velocity field\n",
    "        vcorr_ma[sweep_slice] = v0 + 2*nuw*vp\n",
    "        \n",
    "    return vcorr_ma, out_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/home/pav/repos/vilabella_140530/RAW/CDV140530163005.RAWDLHF'\n",
    "out_path = '/home/pav/repos/vilabella_140530/results_pkl/'\n",
    "\n",
    "file_name = file[-23:-10]\n",
    "outpkl = out_path + file_name + '_corr.pkl'\n",
    "\n",
    "radar = pyart.io.read(f)\n",
    "f = radar.instrument_parameters['prt_ratio']['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/home/pav/repos/vilabella_140530/RAW/'\n",
    "out_path = '/home/pav/repos/vilabella_140530/results_pkl/'\n",
    "\n",
    "for f in glob.glob(data_path + '*.RAW*'):\n",
    "    \n",
    "    plt.close('all')\n",
    "    file_name = f[-23:-10]\n",
    "    outpkl = out_path + file_name + '_corr.pkl'\n",
    "\n",
    "    radar = pyart.io.read(f)\n",
    "    f = radar.instrument_parameters['prt_ratio']['data'][0]\n",
    "    N = int(round(1/(f-1)))\n",
    "\n",
    "    [vcorr_ma, out_mask] = correct_dualPRF_cmean(radar, 'velocity', Nprf=N, Nmin=3)\n",
    "    \n",
    "\n",
    "    radar.add_field_like('velocity','velocity_corr_cmean', vcorr_ma, replace_existing = True)\n",
    "    radar.fields['velocity_corr_cmean']['standard_name'] = 'Corrected Velocity'\n",
    "    radar.fields['velocity_corr_cmean']['units'] = 'm/s'\n",
    "    radar.fields['velocity_corr_cmean']['long_name'] = 'dualPRF filtered Velocity'\n",
    "        \n",
    "    with open(outpkl, 'wb') as output:\n",
    "            pickle.dump(radar, output, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
